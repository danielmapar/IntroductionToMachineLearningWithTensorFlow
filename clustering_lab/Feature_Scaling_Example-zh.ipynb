{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling Example\n",
    "\n",
    "现在你已经看到特征缩放将可能改变 kmeans 算法的聚类，现在让我们动手练习一下吧！\n",
    "\n",
    "首先让我们取一些数据。第一个单元格将读取必要的库，生成数据，并绘制数据的图形，在本 notebook 的其余部分，我们都会使用的这个数据。\n",
    "\n",
    "在本 notebook 中，这个你将一直使用的数据集被存储在变量 **data** 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import tests2 as t\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DSND colors: UBlue, Salmon, Gold, Slate\n",
    "plot_colors = ['#02b3e4', '#ee2e76', '#ffb613', '#2e3d49']\n",
    "\n",
    "# Light colors: Blue light, Salmon light\n",
    "plot_lcolors = ['#88d0f3', '#ed8ca1', '#fdd270']\n",
    "\n",
    "# Gray/bg colors: Slate Dark, Gray, Silver\n",
    "plot_grays = ['#1c262f', '#aebfd1', '#fafbfc']\n",
    "\n",
    "\n",
    "def create_data():\n",
    "    n_points = 120\n",
    "    X = np.random.RandomState(3200000).uniform(-3, 3, [n_points, 2])\n",
    "    X_abs = np.absolute(X)\n",
    "\n",
    "    inner_ring_flag = np.logical_and(X_abs[:,0] < 1.2, X_abs[:,1] < 1.2)\n",
    "    outer_ring_flag = X_abs.sum(axis = 1) > 5.3\n",
    "    keep = np.logical_not(np.logical_or(inner_ring_flag, outer_ring_flag))\n",
    "\n",
    "    X = X[keep]\n",
    "    X = X[:60] # only keep first 100\n",
    "    X1 = np.matmul(X, np.array([[2.5, 0], [0, 100]])) + np.array([22.5, 500])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize = [15,15])\n",
    "\n",
    "    plt.scatter(X1[:,0], X1[:,1], s = 64, c = plot_colors[-1])\n",
    "\n",
    "    plt.xlabel('5k Completion Time (min)', size = 30)\n",
    "    plt.xticks(np.arange(15, 30+5, 5), fontsize = 30)\n",
    "    plt.ylabel('Test Score (raw)', size = 30)\n",
    "    plt.yticks(np.arange(200, 800+200, 200), fontsize = 30)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    [side.set_linewidth(2) for side in ax.spines.values()]\n",
    "    ax.tick_params(width = 2)\n",
    "    plt.savefig('C18_FeatScalingEx_01.png', transparent = True)\n",
    "    \n",
    "    \n",
    "    data = pd.DataFrame(X1)\n",
    "    data.columns = ['5k_Time', 'Raw_Test_Score']\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` 看一下这个数据集。有没有缺失数据？平均完成时间是多少？平均原始测试分数是多少？使用下面的单元格来查找这些问题的答案，使用字典将值与相应的语句匹配，并对照我们的解决方案进行检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another cell for work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionary to match the values to the corresponding statements\n",
    "a = 0\n",
    "b = 60\n",
    "c = 22.9\n",
    "d = 4.53\n",
    "e = 511.7\n",
    "\n",
    "q1_dict = {\n",
    "'number of missing values': # letter here,\n",
    "'the mean 5k time in minutes': # letter here,    \n",
    "'the mean test score as a raw value': # letter here,\n",
    "'number of individuals in the dataset': # letter here\n",
    "}\n",
    "\n",
    "# check your answer against ours here\n",
    "t.check_q1(q1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` 现在，实例化一个有两个聚类的 kmeans `model`。用此模型来 `fit` 和 `predict` 数据集中每个点。将预测结果存储在变量 `preds` 中。如果你正确地创建了模型和预测，那么在运行以下单元格时，你应该会看到上部有一个（蓝色）分类和底部有一个（粉色）分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # instantiate a model with two centers\n",
    "preds = # fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see your results\n",
    "\n",
    "def plot_clusters(data, preds, n_clusters):\n",
    "    plt.figure(figsize = [15,15])\n",
    "\n",
    "    for k, col in zip(range(n_clusters), plot_colors[:n_clusters]):\n",
    "        my_members = (preds == k)\n",
    "        plt.scatter(data['5k_Time'][my_members], data['Raw_Test_Score'][my_members], s = 64, c = col)\n",
    "\n",
    "    plt.xlabel('5k Completion Time (min)', size = 30)\n",
    "    plt.xticks(np.arange(15, 30+5, 5), fontsize = 30)\n",
    "    plt.ylabel('Test Score (raw)', size = 30)\n",
    "    plt.yticks(np.arange(200, 800+200, 200), fontsize = 30)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    [side.set_linewidth(2) for side in ax.spines.values()]\n",
    "    ax.tick_params(width = 2)\n",
    "    \n",
    "plot_clusters(data, preds, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` 现在给你的 `data` 的数据框（dataframe）添加两个新列。第一个是 `test_scaled`，你可以通过减去测试分数的均值并除以测试分数的标准差来创建它。要创建的第二列是 `5k_time_sec`，，它应将分钟更改为秒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here\n",
    "data['test_scaled'] = # standardized test scores\n",
    "data['5k_time_sec'] = # times in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` 现在，与问题 2 类似，实例化一个有两个聚类的 `model`。用你的模型来 `fit` 和 `predict` 数据集中每个点。将预测值存储在变量 `preds` 中。如果你正确地创建了模型和预测，你应该会看到右边有一个（蓝色）分类和左边有一个（粉色）分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # instantiate a model with two centers\n",
    "preds = # fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see your results\n",
    "plot_clusters(data, preds, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` 将最能描述特征缩放方式的变量与使用基于距离的度量或正则化的算法相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "a = 'We should always use normalizing'\n",
    "b = 'We should always scale our variables between 0 and 1.'\n",
    "c = 'Variable scale will frequently influence your results, so it is important to standardize for all of these algorithms.'\n",
    "d = 'Scaling will not change the results of your output.'\n",
    "\n",
    "best_option = # best answer variable here\n",
    "\n",
    "\n",
    "# check your answer against ours here\n",
    "t.check_q5(best_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  如果要参考答案，点击橘色 Jupyter 图标，找到 Feature Scaling Example - Solution 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
